---
title: 大模型1
date: 2025-08-25 22:02:46
tags:
---
Transformer 机制，自注意力机制，可以并行计算序列中所有元素之间的关系。依赖捕捉能力方面具有明显优势。

擅长捕捉远程依赖关系和上下文信息。

微调氛围3个步骤

编码器: 捕捉输入输入序列的语义信息。
解码器: 根据已经处理的输入序列生成新的输出序列。

ChatGPT 对Transformer模型进行了一些列优化
1. 采用多头注意力机制，使得模型能够同时学习不同特征空间的表示，提高了模型性能和泛化能力。
2. 在网络层中采用归一化操作，加速收敛和优化网络参数。
3. 添加位置编码，为不同位置的词汇建立唯一的词向量表示，提高了模型的位置信息识别能力。

微调的3个步骤
1. 人工标注好的数据进行训练。
2. 基于用户对模型生成答案的排序设计一个RM(Reward Model 奖励模型)
3. 通过奖励模型进一步训练ChatGPT

未来发展的方向
1. 多模态大模型
2. 垂直大模型，专业性高，受众较少。

算力估算的公式: 模型大小 * 数据集大小 * 计算平台性能/ 计算时间。

大模型是否包含 CNN RNN DNN GAN 等技术，以及和Transformer之间的区别。
